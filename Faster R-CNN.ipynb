{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import selectivesearch\n",
    "from torchvision import transforms, models, datasets\n",
    "from torchvision.ops import nms\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from glob import glob\n",
    "from random import randint\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.patches as mpatches\n",
    "from PIL import Image\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "IMAGE_ROOT = 'D:\\pycharm\\DL-Pytorch\\Dataset\\open-data-car-truck\\images\\images'\n",
    "DF_RAW = df = pd.read_csv('D:\\pycharm\\DL-Pytorch\\Dataset\\open-data-car-truck\\df.csv')\n",
    "print(DF_RAW.head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "label2target = {l:t+1 for t,l in enumerate(DF_RAW['LabelName'].unique())}\n",
    "label2target['background'] = 0\n",
    "target2label = {t:l for l,t in label2target.items()}\n",
    "background_class = label2target['background']\n",
    "num_classes = len(label2target)"
   ],
   "id": "18c7b2c99d140248",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def preprocess_image(img):\n",
    "    img = torch.tensor(img).permute(2,0,1)\n",
    "    return img.float()\n",
    "\n",
    "def find(item, original_list):\n",
    "    results = []\n",
    "    for o_i in original_list:\n",
    "        if item in o_i:\n",
    "            results.append(o_i)\n",
    "    if len(results) == 1:\n",
    "        return results[0]\n",
    "    else:\n",
    "        return results"
   ],
   "id": "9973370c4dec70a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class OpenDataset(torch.utils.data.Dataset):\n",
    "    w, h = 224, 224\n",
    "    def __init__(self, df, image_dir=IMAGE_ROOT):\n",
    "        self.image_dir = image_dir\n",
    "        self.files = glob(self.image_dir+'/*')\n",
    "        self.df = df\n",
    "        self.image_infos = df.ImageID.unique()\n",
    "    def __getitem__(self, ix):\n",
    "        # load images and masks\n",
    "        image_id = self.image_infos[ix]\n",
    "        img_path = find(image_id, self.files)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = np.array(img.resize((self.w, self.h), resample=Image.BILINEAR))/255.\n",
    "        data = df[df['ImageID'] == image_id]\n",
    "        labels = data['LabelName'].values.tolist()\n",
    "        data = data[['XMin','YMin','XMax','YMax']].values\n",
    "        data[:,[0,2]] *= self.w\n",
    "        data[:,[1,3]] *= self.h\n",
    "        boxes = data.astype(np.uint32).tolist() # convert to absolute coordinates\n",
    "        # torch FRCNN expects ground truths as a dictionary of tensors\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.Tensor(boxes).float()\n",
    "        target[\"labels\"] = torch.Tensor([label2target[i] for i in labels]).long()\n",
    "        img = preprocess_image(img)\n",
    "        return img, target\n",
    "    def collate_fn(self, batch):\n",
    "        return tuple(zip(*batch))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_infos)"
   ],
   "id": "5127ffe40bc9de67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "trn_ids, val_ids = train_test_split(df.ImageID.unique(), test_size=0.1, random_state=99)\n",
    "trn_df, val_df = df[df['ImageID'].isin(trn_ids)], df[df['ImageID'].isin(val_ids)]\n",
    "len(trn_df), len(val_df)\n",
    "\n",
    "train_ds = OpenDataset(trn_df)\n",
    "test_ds = OpenDataset(val_df)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, collate_fn=train_ds.collate_fn, drop_last=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=4, collate_fn=test_ds.collate_fn, drop_last=True)"
   ],
   "id": "937c02b557e7acfe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def get_model():\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model"
   ],
   "id": "7b1b9cb7df29ffa8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_batch(inputs, model, optimizer):\n",
    "    model.train()\n",
    "    input, targets = inputs\n",
    "    input = list(image.to(device) for image in input)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    optimizer.zero_grad()\n",
    "    losses = model(input, targets)\n",
    "    loss = sum(loss for loss in losses.values())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, losses\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_batch(inputs, model, optimizer):\n",
    "    model.train()\n",
    "    input, targets = inputs\n",
    "    input = list(image.to(device) for image in input)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    optimizer.zero_grad()\n",
    "    losses = model(input, targets)\n",
    "    loss = sum(loss for loss in losses.values())\n",
    "    return loss, losses"
   ],
   "id": "736f8c19ca047c09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = get_model().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "n_epochs = 10\n",
    "\n",
    "train_loss_epochs = []\n",
    "train_loc_loss_epochs = []\n",
    "train_regr_loss_epochs = []\n",
    "train_objectness_loss_epochs = []\n",
    "train_rpn_box_reg_loss_epochs = []\n",
    "val_loss_epochs = []\n",
    "val_loc_loss_epochs = []\n",
    "val_regr_loss_epochs = []\n",
    "val_objectness_loss_epochs = []\n",
    "val_rpn_box_reg_loss_epochs = []"
   ],
   "id": "64a29cf112b7043a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for epoch in range(n_epochs):\n",
    "    print(epoch)\n",
    "    _n = len(train_loader)\n",
    "    trn_loss = []\n",
    "    trn_loc_loss = []\n",
    "    trn_regr_loss = []\n",
    "    trn_objectness_loss = []\n",
    "    trn_rpn_box_reg_loss = []\n",
    "    val_loss = []\n",
    "    val_loc_loss = []\n",
    "    val_regr_loss = []\n",
    "    val_objectness_loss = []\n",
    "    val_rpn_box_reg_loss = []\n",
    "    for ix, inputs in enumerate(train_loader):\n",
    "        loss, losses = train_batch(inputs, model, optimizer)\n",
    "        loc_loss, regr_loss, loss_objectness, loss_rpn_box_reg = \\\n",
    "            [losses[k] for k in ['loss_classifier','loss_box_reg','loss_objectness','loss_rpn_box_reg']]\n",
    "        pos = (epoch + (ix+1)/_n)\n",
    "        trn_loss.append(loss.item())\n",
    "        trn_loc_loss.append(loc_loss.item())\n",
    "        trn_regr_loss.append(regr_loss.item())\n",
    "        trn_objectness_loss.append(loss_objectness.item())\n",
    "        trn_rpn_box_reg_loss.append(loss_rpn_box_reg.item())\n",
    "    train_loss_epochs.append(np.average(trn_loss))\n",
    "    train_loc_loss_epochs.append(np.average(trn_loc_loss))\n",
    "    train_regr_loss_epochs.append(np.average(trn_regr_loss))\n",
    "    train_objectness_loss_epochs.append(np.average(trn_objectness_loss))\n",
    "    train_rpn_box_reg_loss_epochs.append(np.average(trn_rpn_box_reg_loss))\n",
    "\n",
    "    _n = len(test_loader)\n",
    "    for ix,inputs in enumerate(test_loader):\n",
    "        loss, losses = validate_batch(inputs, model, optimizer)\n",
    "        loc_loss, regr_loss, loss_objectness, loss_rpn_box_reg = \\\n",
    "          [losses[k] for k in ['loss_classifier','loss_box_reg','loss_objectness','loss_rpn_box_reg']]\n",
    "        pos = (epoch + (ix+1)/_n)\n",
    "        val_loss.append(loss.item())\n",
    "        val_loc_loss.append(loc_loss.item())\n",
    "        val_regr_loss.append(regr_loss.item())\n",
    "        val_objectness_loss.append(loss_objectness.item())\n",
    "        val_rpn_box_reg_loss.append(loss_rpn_box_reg.item())\n",
    "    val_loss_epochs.append(np.average(val_loss))\n",
    "    val_loc_loss_epochs.append(np.average(val_loc_loss))\n",
    "    val_regr_loss_epochs.append(np.average(val_regr_loss))\n",
    "    val_objectness_loss_epochs.append(np.average(val_objectness_loss))\n",
    "    val_rpn_box_reg_loss_epochs.append(np.average(val_rpn_box_reg_loss))"
   ],
   "id": "74d09a1e2b739e26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "epochs = np.arange(n_epochs)+1\n",
    "plt.plot(epochs, train_loss_epochs, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_epochs, 'r', label='Test loss')\n",
    "plt.title('Training and Test loss over increasing epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid('off')\n",
    "plt.show()"
   ],
   "id": "2a70c224b649897c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torchvision.ops import nms\n",
    "def decode_output(output):\n",
    "    'convert tensors to numpy arrays'\n",
    "    bbs = output['boxes'].cpu().detach().numpy().astype(np.uint16)\n",
    "    labels = np.array([target2label[i] for i in output['labels'].cpu().detach().numpy()])\n",
    "    confs = output['scores'].cpu().detach().numpy()\n",
    "    ixs = nms(torch.tensor(bbs.astype(np.float32)), torch.tensor(confs), 0.05)\n",
    "    bbs, confs, labels = [tensor[ixs] for tensor in [bbs, confs, labels]]\n",
    "\n",
    "    if len(ixs) == 1:\n",
    "        bbs, confs, labels = [np.array([tensor]) for tensor in [bbs, confs, labels]]\n",
    "    return bbs.tolist(), confs.tolist(), labels.tolist()\n",
    "\n",
    "# print(clss)\n",
    "def show_bbs(im, bbs, clss):\n",
    "    fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(6, 6))\n",
    "    ax[0].imshow(im)\n",
    "    ax[0].grid(False)\n",
    "    ax[0].set_title('Original image')\n",
    "    if len(bbs) == 0:\n",
    "        ax[1].imshow(im)\n",
    "        ax[1].set_title('No objects')\n",
    "        plt.show()\n",
    "        return\n",
    "    ax[1].imshow(im)\n",
    "    for ix, (xmin, ymin, xmax, ymax) in enumerate(bbs):\n",
    "        rect = mpatches.Rectangle(\n",
    "                (xmin, ymin), xmax-xmin, ymax-ymin,\n",
    "                fill=False,\n",
    "                edgecolor='red',\n",
    "                linewidth=1)\n",
    "        ax[1].add_patch(rect)\n",
    "        centerx = xmin # + new_w/2\n",
    "        centery = ymin + 20# + new_h - 10\n",
    "        plt.text(centerx, centery, clss[ix],fontsize = 20,color='red')\n",
    "    ax[1].grid(False)\n",
    "    ax[1].set_title('Predicted bounding box and class')\n",
    "    plt.show()"
   ],
   "id": "8fe7f6463ee72330",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "for ix, (images, targets) in enumerate(test_loader):\n",
    "    if ix==20:\n",
    "        break\n",
    "    images = [im for im in images]\n",
    "    outputs = model(images)\n",
    "    for ix, output in enumerate(outputs):\n",
    "        bbs, confs, labels = decode_output(output)\n",
    "        info = [f'{l}@{c:.2f}' for l,c in zip(labels, confs)]\n",
    "        show_bbs(images[ix].cpu().permute(1,2,0), bbs=bbs, clss=labels)"
   ],
   "id": "51884e525d3f82f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
