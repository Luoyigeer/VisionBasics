{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-31T04:51:07.221241Z",
     "start_time": "2025-10-31T04:51:01.844545Z"
    }
   },
   "source": [
    "from torchvision.ops import nms\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from glob import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.patches as mpatches\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "DATA_ROOT = 'D:\\pycharm\\DL-Pytorch\\Dataset\\open-data-car-truck\\images/'\n",
    "IMAGE_ROOT = f'{DATA_ROOT}/images'\n",
    "DF_RAW = df = pd.read_csv('D:\\pycharm\\DL-Pytorch\\Dataset\\open-data-car-truck\\df.csv')\n",
    "print(DF_RAW.head())\n",
    "\n",
    "df = df[df['ImageID'].isin(df['ImageID'].unique().tolist())]\n",
    "label2target = {l:t+1 for t,l in enumerate(DF_RAW['LabelName'].unique())}\n",
    "label2target['background'] = 0\n",
    "target2label = {t:l for l,t in label2target.items()}\n",
    "background_class = label2target['background']\n",
    "num_classes = len(label2target)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ImageID  Source LabelName  Confidence      XMin      XMax  \\\n",
      "0  0000599864fd15b3  xclick       Bus           1  0.343750  0.908750   \n",
      "1  00006bdb1eb5cd74  xclick     Truck           1  0.276667  0.697500   \n",
      "2  00006bdb1eb5cd74  xclick     Truck           1  0.702500  0.999167   \n",
      "3  00010bf498b64bab  xclick       Bus           1  0.156250  0.371250   \n",
      "4  00013f14dd4e168f  xclick       Bus           1  0.287500  0.999375   \n",
      "\n",
      "       YMin      YMax  IsOccluded  IsTruncated  ...  IsDepiction  IsInside  \\\n",
      "0  0.156162  0.650047           1            0  ...            0         0   \n",
      "1  0.141604  0.437343           1            0  ...            0         0   \n",
      "2  0.204261  0.409774           1            1  ...            0         0   \n",
      "3  0.269188  0.705228           0            0  ...            0         0   \n",
      "4  0.194184  0.999062           0            1  ...            0         0   \n",
      "\n",
      "   XClick1X  XClick2X  XClick3X  XClick4X  XClick1Y  XClick2Y  XClick3Y  \\\n",
      "0  0.421875  0.343750  0.795000  0.908750  0.156162  0.512700  0.650047   \n",
      "1  0.299167  0.276667  0.697500  0.659167  0.141604  0.241855  0.352130   \n",
      "2  0.849167  0.702500  0.906667  0.999167  0.204261  0.398496  0.409774   \n",
      "3  0.274375  0.371250  0.311875  0.156250  0.269188  0.493882  0.705228   \n",
      "4  0.920000  0.999375  0.648750  0.287500  0.194184  0.303940  0.999062   \n",
      "\n",
      "   XClick4Y  \n",
      "0  0.457197  \n",
      "1  0.437343  \n",
      "2  0.295739  \n",
      "3  0.521691  \n",
      "4  0.523452  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T04:51:07.237230Z",
     "start_time": "2025-10-31T04:51:07.228238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "denormalize = transforms.Normalize(\n",
    "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.255],\n",
    "    std=[1/0.229, 1/0.224, 1/0.255]\n",
    ")\n",
    "def preprocess_image(img):\n",
    "    img = torch.tensor(img).permute(2,0,1)\n",
    "    img = normalize(img)\n",
    "    return img.to(device).float()\n",
    "\n",
    "def find(item, original_list):\n",
    "    results = []\n",
    "    for o_i in original_list:\n",
    "        if item in o_i:\n",
    "            results.append(o_i)\n",
    "    if len(results) == 1:\n",
    "        return results[0]\n",
    "    else:\n",
    "        return results"
   ],
   "id": "fef2761f1c17a347",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T04:51:07.518398Z",
     "start_time": "2025-10-31T04:51:07.244227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class OpenDataset(torch.utils.data.Dataset):\n",
    "    w, h = 300, 300\n",
    "    def __init__(self, df, image_dir=IMAGE_ROOT):\n",
    "        self.image_dir = image_dir\n",
    "        self.files = glob(self.image_dir+'/*')\n",
    "        self.df = df\n",
    "        self.image_infos = df.ImageID.unique()\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        # load images and masks\n",
    "        image_id = self.image_infos[ix]\n",
    "        img_path = find(image_id, self.files)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = np.array(img.resize((self.w, self.h), resample=Image.BILINEAR))/255.\n",
    "        data = df[df['ImageID'] == image_id]\n",
    "        labels = data['LabelName'].values.tolist()\n",
    "        data = data[['XMin','YMin','XMax','YMax']].values\n",
    "        data[:,[0,2]] *= self.w\n",
    "        data[:,[1,3]] *= self.h\n",
    "        boxes = data.astype(np.uint32).tolist() # convert to absolute coordinates\n",
    "        return img, boxes, labels\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        images, boxes, labels = [], [], []\n",
    "        for item in batch:\n",
    "            img, image_boxes, image_labels = item\n",
    "            img = preprocess_image(img)[None]\n",
    "            images.append(img)\n",
    "            boxes.append(torch.tensor(image_boxes).float().to(device)/300.)\n",
    "            labels.append(torch.tensor([label2target[c] for c in image_labels]).long().to(device))\n",
    "        images = torch.cat(images).to(device)\n",
    "        return images, boxes, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_infos)"
   ],
   "id": "a7336f2ad32188aa",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T04:51:08.422992Z",
     "start_time": "2025-10-31T04:51:07.528398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "trn_ids, val_ids = train_test_split(df.ImageID.unique(), test_size=0.1, random_state=99)\n",
    "trn_df, val_df = df[df['ImageID'].isin(trn_ids)], df[df['ImageID'].isin(val_ids)]\n",
    "len(trn_df), len(val_df)\n",
    "\n",
    "train_ds = OpenDataset(trn_df)\n",
    "test_ds = OpenDataset(val_df)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, collate_fn=train_ds.collate_fn, drop_last=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=4, collate_fn=test_ds.collate_fn, drop_last=True)"
   ],
   "id": "c04d3bc620d7c88a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T04:51:08.438998Z",
     "start_time": "2025-10-31T04:51:08.428975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_batch(inputs, model, criterion, optimizer):\n",
    "    model.train()\n",
    "    N = len(train_loader)\n",
    "    images, boxes, labels = inputs\n",
    "    _regr, _clss = model(images)\n",
    "    loss = criterion(_regr, _clss, boxes, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_batch(inputs, model, criterion):\n",
    "    model.eval()\n",
    "    images, boxes, labels = inputs\n",
    "    _regr, _clss = model(images)\n",
    "    loss = criterion(_regr, _clss, boxes, labels)\n",
    "    return loss"
   ],
   "id": "f303e32a15811b0c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T04:51:11.188958Z",
     "start_time": "2025-10-31T04:51:08.446964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from model import SSD300, MultiBoxLoss\n",
    "from detect import *\n",
    "\n",
    "model = SSD300(num_classes, device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "criterion = MultiBoxLoss(priors_cxcy=model.priors_cxcy, device=device)"
   ],
   "id": "321eb178a795a921",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\pytorch_new\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\anaconda\\envs\\pytorch_new\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded base model.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\pytorch_new\\lib\\site-packages\\torch\\nn\\_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-10-31T04:51:16.304682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loss_epochs = []\n",
    "val_loss_epochs = []\n",
    "n_epochs=10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(epoch)\n",
    "    _n = len(train_loader)\n",
    "    trn_loss = []\n",
    "    val_loss = []\n",
    "    for ix, inputs in enumerate(train_loader):\n",
    "        loss = train_batch(inputs, model, criterion, optimizer)\n",
    "        pos = (epoch + (ix+1)/_n)\n",
    "        trn_loss.append(loss.item())\n",
    "    train_loss_epochs.append(np.average(trn_loss))\n",
    "\n",
    "    _n = len(test_loader)\n",
    "    for ix,inputs in enumerate(test_loader):\n",
    "        loss = validate_batch(inputs, model, criterion)\n",
    "        pos = (epoch + (ix+1)/_n)\n",
    "        val_loss.append(loss.item())\n",
    "val_loss_epochs.append(np.average(val_loss))\n",
    "\n",
    "epochs = np.arange(n_epochs)+1\n",
    "plt.plot(epochs, train_loss_epochs, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_epochs, 'r', label='Test loss')\n",
    "plt.title('Training and Test loss over increasing epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid('off')\n",
    "plt.show()"
   ],
   "id": "3ebd7edb9cf9f1f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def show_bbs(im, bbs, clss):\n",
    "    fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(6, 6))\n",
    "    ax[0].imshow(im)\n",
    "    ax[0].grid(False)\n",
    "    ax[0].set_title('Original image')\n",
    "    if len(bbs) == 0:\n",
    "        ax[1].imshow(im)\n",
    "        ax[1].set_title('No objects')\n",
    "        plt.show()\n",
    "        return\n",
    "    ax[1].imshow(im)\n",
    "    for ix, (xmin, ymin, xmax, ymax) in enumerate(bbs):\n",
    "        rect = mpatches.Rectangle(\n",
    "                (xmin, ymin), xmax-xmin, ymax-ymin,\n",
    "                fill=False,\n",
    "                edgecolor='red',\n",
    "                linewidth=1)\n",
    "        ax[1].add_patch(rect)\n",
    "        centerx = xmin # + new_w/2\n",
    "        centery = ymin + 20# + new_h - 10\n",
    "        plt.text(centerx, centery, clss[ix].replace('@', ''),fontsize = 10,color='red')\n",
    "    ax[1].grid(False)\n",
    "    ax[1].set_title('Predicted bounding box and class')\n",
    "    plt.show()\n",
    "\n",
    "from random import choice\n",
    "image_paths = glob.glob(f'{DATA_ROOT}/images/*')\n",
    "image_id = choice(test_ds.image_infos)\n",
    "print(image_id)\n",
    "img_path = find(image_id, test_ds.files)\n",
    "original_image = Image.open(img_path, mode='r')\n",
    "original_image = original_image.convert('RGB')"
   ],
   "id": "1b61251d1a0fb194"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "image_paths = glob.glob(f'{DATA_ROOT}/images/*')\n",
    "for _ in range(20):\n",
    "    image_id = choice(test_ds.image_infos)\n",
    "    img_path = find(image_id, test_ds.files)\n",
    "    original_image = Image.open(img_path, mode='r')\n",
    "    bbs, labels, scores = detect(original_image, model, min_score=0.9, max_overlap=0.5,top_k=200, device=device)\n",
    "    labels = [target2label[c.item()] for c in labels]\n",
    "    label_with_conf = [f'{l} @ {s:.2f}' for l,s in zip(labels,scores)]\n",
    "    print(bbs, label_with_conf)\n",
    "    show_bbs(original_image, bbs=bbs, clss=label_with_conf)#, text_sz=10)"
   ],
   "id": "4182d5e518315217"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
